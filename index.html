<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ViTaB-A: EVALUATING MULTIMODAL LARGE LANGUAGE MODELS ON VISUAL TABLE ATTRIBUTION">
  <meta name="keywords" content="Benchmark, Evaluation, MLLM, Vision-Language, Attribution, Uncertainty">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ViTaB-A</title>

  <!-- <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;600;700&family=Fraunces:opsz,wght@9..144,400;9..144,600&family=Raleway:wght@400;500;600;700&display=swap"
        rel="stylesheet"> -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/lowell.jpeg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero hero-landing">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <!-- <img src="./static/images/lowell.jpeg" alt="Logo" class="publication-logo"> -->
            ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution
          </h1>
          <!-- <p class="subtitle is-4 hero-subtitle">
            Characterizing robustness and grounding in multimodal large language models (MLLMs)
            through perception-first benchmarks and structured perturbations.
          </p> -->

          <div class="hero-pills">
            <span class="pill">Attribution</span>
            <span class="pill">MLLM</span>
            <span class="pill">Uncertainty</span>
            <span class="pill">Benchmark Suite</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Yahialqur">Yahia Alqurnawi</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=yTXdQ28AAAAJ&hl=en&oi=ao">Preetom Biswas</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/raoanmol">Anmol Rao</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://tejasanvekar.github.io/">Tejas Anvekar</a>,</span>
            <span class="author-block">
              <a href="https://chitta.orissalinks.com/www/">Chitta Baral</a>,</span>
            <span class="author-block">
              <a href="https://vgupta123.github.io/">Vivek Gupta</a>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block asu-affiliation">Arizona State University</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution.</small></span>
          </div>
          <div class="wacv-line">ICLR 2026 (Under Review)</div>

          <div class="hero-cta">
            <a href="./static/images/Benchmarking_and_Evaluating_Visual_Table_Attribution_Beyond_F_measure.pdf"
               class="button is-rounded is-dark">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper</span>
            </a>
            <a href=""
               class="button is-rounded is-light">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
            <a href="https://github.com/Yahialqur/ViTAB-A"
               class="button is-rounded is-light">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <!-- <a href=""
               class="button is-rounded is-light">
              <span class="icon">ðŸ¤—</span>
              <span>Dataset (Coming Soon)</span>
            </a> -->
          </div>

          <div class="hero-highlight">
            <div class="highlight-card">
              <p class="highlight-title">What it explores</p>
              
              <p class="highlight-text">
                How accurately do mLLMs identify table cells that support a given answer?
              </p>
              <br/>
              
              <p class="highlight-text">
                Does a model's confidence score reliably reflect the correctness of its attribution?
              </p>
            </div>
            <div class="highlight-card">
              <p class="highlight-title">What it delivers</p>
              <p class="highlight-text">
                A unified benchmark for structured data attribution that evaluates question answering,
                fine-grained row and column localization, and confidence calibration across text, JSON,
                and image-based tablesâ€”providing a clear measure of grounding and traceability.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="introduction">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-title has-text-centered">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (mLLMs) can answer questions over structured data such as tables and JSON with reasonable accuracy, but they often fail to identify the exact rows and columns that support their answers. This gap between answer correctness and evidence localization limits their reliability in settings that require traceability. To address this, we introduce <b>ViTaB-A</b>, a benchmark designed to evaluate structured data attribution across text, JSON, and rendered table images. ViTaB-A systematically measures whether models can not only answer correctly, but also precisely ground their responses in the underlying table fields.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section section-contrast" id="contributions">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title has-text-centered">Major Contributions</h2>
    <div class="columns is-multiline">
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5"> Representation-Wise Evaluation</h3>
          <p>
            A comprehensive benchmark for evaluating multimodal LLMs on visual table attribution across text tables, JSON files, and rendered table images.
          </p>
        </div>
      </div>
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Unified Tasks + Metrics</h3>
          <p>
            The first large-scale evaluation of open-source mLLMs that jointly measures table question
            answering, fine-grained row/column attribution, and confidence alignment.
          </p>
        </div>
      </div>
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Spatial Grounding Analysis</h3>
          <p>
            Empirical analysis revealing systematic gaps between answer accuracy and evidence
            localization, highlighting weaknesses in spatial grounding and traceability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="observatory">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3 section-title">ViTaB-A Pipeline</h2>
        <p class="content">
          ViTaB-A provides a structured evaluation setup consisting of questionâ€“answer pairs, table representations (JSON, Markdown, or rendered images), and ground-truth row and column citations. These inputs are evaluated under multiple prompting paradigmsâ€”including zero-shot, few-shot, and chain-of-thoughtâ€”to systematically assess attribution quality. Performance is measured not only through answer and citation accuracy, but also through model certainty and calibration metrics, enabling a comprehensive analysis of grounding reliability.
        </p>
        <figure class="overview-figure">
          <img src="./static/images/flow_diagram.png"
               alt="Framework overview of ViTaB-A Dataset Generation and Evaluation Pipeline">
          <figcaption class="is-size-6 has-text-grey">
            Framework overview of ViTaB-A Dataset Generation and Evaluation Pipeline.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section section-contrast" id="data">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered">
      <div class="column is-6">
        <h2 class="title is-3 section-title">Benchmark Details</h2>
        <p class="content">
          ViTaB-A evaluates structured data attribution using curated questionâ€“answer pairs
          grounded in tabular data with explicit row and column citations. Each instance
          includes a table and its corresponding ground-truth supporting cells.
        </p>
        <p class="content">
          Tables are presented across multiple modalities to assess
          robustness of attribution under representational and visual variation. For Image-based representation, 
          additional visual perturbations (such as varying header color, font-style) are added for generalized evaluation.
        </p>
      </div>
      <div class="column is-6">
          <div class="metric">
            <p class="metric-label">Table Formats</p>
            <p class="metric-value">JSON Â· Markdown Â· Rendered Images</p>
          </div>
           <br/>
          <div class="metric">
            <p class="metric-label">Image Perturbation</p>
            <p class="metric-value">Red Â· Green Â· Blue Â· Arial Â· Times New Roman</p>
          </div>
          <!-- <div class="metric">
            <p class="metric-label">Variants</p>
            <p class="metric-value">15 ID + 15 OOD</p>
          </div>
          <div class="metric">
            <p class="metric-label">Total</p>
            <p class="metric-value">62K images</p>
          </div> -->
      </div>
    </div>
  </div>
</section>

<section class="section" id="tasks">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title has-text-centered">Evaluation Tasks</h2>
    <div class="content has-text-justified">
      <p>
        ViTaB-A evaluates multimodal LLMs along two complementary dimensions:
        answer and attribution correctness, and the alignment of model certainty.
      </p>
    </div>
    <div class="task-stack">
      <div class="task-panel">
        <div class="task-panel-text">
          <h3 class="title is-4">Task 1: QA & Attribution Accuracy</h3>
          <p>
            Models are evaluated on their ability to generate the correct answer
            and to precisely identify the supporting table cells (row and column).
            We measure answer accuracy, row accuracy, column accuracy, and cell accuracy to quantify fine-grained grounding performance
            across JSON, Markdown, and rendered table inputs.
          </p>
        </div>
        <!-- <figure class="task-panel-figure ">
          <div class="metric">
            <p class="metric-label">Model Families</p>
            <p class="metric-value">Qwen3-VL  â†’ 2B Â· 4B Â· 8B Â· 32B</p>
            <p class="metric-value">Gemma3  â†’ 4B Â· 12B Â· 27B</p>
            <p class="metric-value">Molmo2  â†’ 4B Â· 8B</p>
            <p class="metric-value">InternVL-3.5  â†’ 4B Â· 8B Â· 14B Â· 38B</p>
          </div>
          <div class="metric">
            <p class="metric-label">Prompting Techniques</p>
            <p class="metric-value">Zero-shot Â· Few-shot Â· Chain-of-Thought </p>
          </div>
        </figure> -->
      </div>
      <div class="task-panel">
        <div class="task-panel-text">
          <h3 class="title is-4">Task 2: Internal & Verbalized Certainty</h3>
          <p>
            Beyond correctness, we assess whether a model's expressed confidence
            aligns with its internal certainty measure. We compare verbalized certainty
            (self-reported confidence scores) with internal token probability-based
            measures to evaluate calibration and uncertainty alignment in
            structured attribution settings.
          </p>
        </div>
      </div>
      <div class="task-panel">
        <div class="task-panel-text">
          <h3 class="title is-4">Task 3: Confidence-Accuracy Alignment</h3>
          <p>
            Beyond correctness, we assess whether a model's confidence (both internal and verbalized) can be a reliable indicator of its true accuracy . We measure alignment through Brier Score for accuracy and confidence. Higher alignment would denote that confidence is an acceptable reflection of the model's performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="properties">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title has-text-centered">Properties Measured</h2>
    <div class="content has-text-justified">
      <p>
        ViTaB-A measures both grounding correctness and uncertainty reliability.
        We evaluate fine-grained attribution performance at the cell level and
        assess whether a modelâ€™s expressed confidence aligns with its true accuracy.
      </p>
    </div>
    <div class="columns is-multiline">
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Cell Accuracy</h3>
          <p>Percentage of predictions where both the predicted row and column
            exactly match the ground-truth supporting cell.</p>
        </div>
      </div>
      
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Row Accuracy</h3>
          <p>
            Percentage of instances where the predicted row matches the
            ground-truth row, isolating record-level localization ability.
          </p>
        </div>
      </div>

      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Column Accuracy</h3>
          <p>
            Percentage of instances where the predicted column matches the
            ground-truth column, isolating field-level localization ability.
          </p>
        </div>
      </div>
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Internal Confidence</h3>
          <p>
            Probability-based confidence derived from the modelâ€™s token-level
            output distribution for its predicted answer or citation.
          </p>
        </div>
      </div>

      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Verbalized Confidence</h3>
          <p>
            Self-reported confidence score of the given attribution response explicitly evaluated by the model.
          </p>
        </div>
      </div>
      <div class="column is-4">
        <div class="content card-panel">
          <h3 class="title is-5">Alignment Score</h3>
          <p>Calibration between confidence and correctness measured via the Brier Score,
      computed as the mean squared difference between confidence and accuracy.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section section-contrast" id="experiments">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered">
      <div class="column is-6">
        <h2 class="title is-3 section-title">Experiments & Setup</h2>
        <p class="content">
          The evaluation spans four model families where each model is evaluated on the ViTaB-A benchmark through three prompting paradigms: zero-shot, few-shot, chain-of-thought.
        </p>
      </div>
      <div class="column is-6">
        <div class="callout">
          <h3 class="title is-5">Model Families</h3>
          <div class="callout-tags">
            <span class="pill pill-outline">Qwen3-VL  â†’ 2B Â· 4B Â· 8B Â· 32B</span>
            <span class="pill pill-outline">Gemma3  â†’ 4B Â· 12B Â· 27B</span>
            <span class="pill pill-outline">Molmo2  â†’ 4B Â· 8B</span>
            <span class="pill pill-outline">InternVL-3.5  â†’ 4B Â· 8B Â· 14B Â· 38B</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="results">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered">
      <div class="column is-6">
        <h2 class="title is-3 section-title">Results & Discussion</h2>
        <p class="content">
          Four consistent themes emerge from our evaluation: QA accuracy remains moderate (â‰ˆ50â€“60%) across models and formats, but attribution accuracy drops sharplyâ€”near random for JSON and only ~30% for images. Attribution is consistently stronger for image-based tables than textual formats, revealing a modality-dependent grounding gap. Models are substantially better at identifying rows than columns, exposing weaknesses in fine-grained field localization. Finally, confidenceâ€”both internal and verbalizedâ€”shows no consistent alignment with attribution accuracy, indicating that certainty is an unreliable signal of grounding quality.

        </p>
      </div>
      <div class="column is-6">
        <div class="callout">
          <h3 class="title is-5">Implications</h3>
          <p class="content">
            Current results reveal a structural gap between answer generation and attribution quality, exposing where multimodal LLMs produce correct responses without reliable grounding in table fields. ViTaB-A provides a effective pipeline for evaluating traceability and confidence reliability in structured data settings.
          </p>
          <div class="callout-tags">
            <span class="pill pill-outline">QAâ€“Attribution Gap</span>
            <span class="pill pill-outline">Localization Failures</span>
            <span class="pill pill-outline">Unreliable Confidence Measures</span>
          </div>
        </div>
      </div>
    </div>
    <div class="results-stack">
      <figure class="figure-card full-figure">
        <img src="./static/images/qa-attr.png" alt="Figure 5: Robustness vs gap for Task 3(a)">
        <figcaption class="is-size-6 has-text-grey">
          Table 1: Model Accuracy in QA vs Attribution (%) Across Prompting Strategies and Open-source Models (4B). Note: <span style="color:ForestGreen;">green</span> indicates the overall best model and
    <span style="color:red;">red</span> indicates the worst.
        </figcaption>
      </figure>
      <figure class="figure-card full-figure">
        <img src="./static/images/row-col.png" alt="Table 1: Robustness of MLLMs for ID vs OOD across tasks">
        <figcaption class="is-size-6 has-text-grey">
          Table 2: Row vs Column Accuracy (%) Across Modalities and Prompting Strategies and Open-source Models (for 4B Parameter). Note: <span style="color:ForestGreen;">green</span> indicates the overall best model and
    <span style="color:red;">red</span> indicates the worst.
        </figcaption>
      </figure>
      <figure class="figure-card full-figure">
        <img src="./static/images/conf-acc.png" alt="Figure 6: Multidimensional insights for Task 2 across datasets">
        <figcaption class="is-size-6 has-text-grey">
          Table 3: Confidence-Accuracy correlation for Internal and Verbal; Across Multiple Modalities. Results show no direct correlation.
        </figcaption>
      </figure>
      <figure class="figure-card full-figure">
        <img src="./static/images/radar.png" alt="Figure 7: Celeb chain length vs. outcome">
        <figcaption class="is-size-6 has-text-grey">
          Figure 2: Radar charts comparing model families across attribution accuracy, QA accuracy, and confidence gap (1 - |verbal - internal|) under different prompting strategies. InternVL-3.5 shows best performance.
        </figcaption>
      </figure>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title section-title">BibTeX</h2>
    <pre><code>@misc{,
      title={ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution}, 
      author={Yahia Alqurnawi and Preetom Biswas and Anmol Rao and Tejas Anvekar and Chitta Baral and Vivek Gupta},
      year={2026},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/}, 
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was adopted from the <a href="https://coral-lab-asu.github.io/PerceptualObservatory/">Perceptual Observatory</a> project page.
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            This means you are free to borrow the source code of this website, we just ask that you link
            back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
